#!/usr/bin/env python3
"""
Titanicã‚³ãƒ³ãƒšé¢¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ - Perishedäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
ãƒªãƒ¼ã‚¯è¨±å®¹ãƒ»train+testçµåˆãƒ»ãƒ•ãƒ«ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings('ignore')

# =====================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# =====================================
print("ğŸ“Š Loading data...")
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

print(f"Train shape: {train.shape}")
print(f"Test shape: {test.shape}")
print(f"\nTarget distribution:\n{train['Perished'].value_counts()}")

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®IDä¿å­˜
train_len = len(train)
test_ids = test['PassengerId'].copy()

# =====================================
# train + test çµåˆï¼ˆãƒªãƒ¼ã‚¯è¾¼ã¿ï¼‰
# =====================================
print("\nğŸ”— Combining train and test datasets...")
y_train = train['Perished'].copy()
train_drop = train.drop('Perished', axis=1)
full = pd.concat([train_drop, test], axis=0, ignore_index=True)
print(f"Full dataset shape: {full.shape}")

# =====================================
# ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆfullãƒ™ãƒ¼ã‚¹ï¼‰
# =====================================
print("\nâš™ï¸  Feature engineering (leak-inclusive)...")

# 1. TitleæŠ½å‡º
full['Title'] = full['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)
# ã‚¿ã‚¤ãƒˆãƒ«ã‚’çµ±åˆ
title_mapping = {
    'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',
    'Dr': 'Rare', 'Rev': 'Rare', 'Col': 'Rare', 'Major': 'Rare', 'Mlle': 'Miss',
    'Countess': 'Rare', 'Ms': 'Miss', 'Lady': 'Rare', 'Jonkheer': 'Rare',
    'Don': 'Rare', 'Dona': 'Rare', 'Mme': 'Mrs', 'Capt': 'Rare', 'Sir': 'Rare'
}
full['Title'] = full['Title'].map(title_mapping).fillna('Rare')
print(f"  - Title: {full['Title'].nunique()} categories")

# 2. FamilySize
full['FamilySize'] = full['SibSp'] + full['Parch'] + 1
full['IsAlone'] = (full['FamilySize'] == 1).astype(int)
print(f"  - FamilySize: range {full['FamilySize'].min()}-{full['FamilySize'].max()}")

# 3. TicketPrefixæŠ½å‡º
full['TicketPrefix'] = full['Ticket'].str.extract('([A-Za-z/\.]+)', expand=False)
full['TicketPrefix'] = full['TicketPrefix'].fillna('NONE')
# é »åº¦ãŒå°‘ãªã„ã‚‚ã®ã‚’ã¾ã¨ã‚ã‚‹
ticket_counts = full['TicketPrefix'].value_counts()
full['TicketPrefix'] = full['TicketPrefix'].apply(
    lambda x: x if ticket_counts[x] >= 5 else 'RARE'
)
print(f"  - TicketPrefix: {full['TicketPrefix'].nunique()} categories")

# 4. CabinLetteræŠ½å‡º
full['CabinLetter'] = full['Cabin'].str[0]
full['CabinLetter'] = full['CabinLetter'].fillna('X')  # æ¬ æã¯X
full['HasCabin'] = (full['Cabin'].notna()).astype(int)
print(f"  - CabinLetter: {full['CabinLetter'].nunique()} categories")

# 5. æ¬ æå€¤å‡¦ç†ï¼ˆfullãƒ™ãƒ¼ã‚¹ï¼‰
# Age: ã‚¿ã‚¤ãƒˆãƒ«åˆ¥ã®ä¸­å¤®å€¤ã§åŸ‹ã‚ã‚‹ï¼ˆãƒªãƒ¼ã‚¯è¾¼ã¿ï¼‰
age_title_median = full.groupby('Title')['Age'].transform('median')
full['Age'] = full['Age'].fillna(age_title_median)

# Fare: ä¸­å¤®å€¤ã§åŸ‹ã‚ã‚‹ï¼ˆfullãƒ™ãƒ¼ã‚¹ï¼‰
full['Fare'] = full['Fare'].fillna(full['Fare'].median())

# Embarked: æœ€é »å€¤ã§åŸ‹ã‚ã‚‹
full['Embarked'] = full['Embarked'].fillna(full['Embarked'].mode()[0])

print(f"  - Age filled: {full['Age'].isnull().sum()} missing")
print(f"  - Fare filled: {full['Fare'].isnull().sum()} missing")
print(f"  - Embarked filled: {full['Embarked'].isnull().sum()} missing")

# 6. Age/Fare ãƒ“ãƒ‹ãƒ³ã‚°
full['AgeBin'] = pd.cut(full['Age'], bins=[0, 12, 18, 35, 60, 100],
                        labels=['Child', 'Teen', 'Adult', 'Middle', 'Senior'])
full['FareBin'] = pd.qcut(full['Fare'], q=5, labels=['VeryLow', 'Low', 'Med', 'High', 'VeryHigh'],
                          duplicates='drop')
# ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å‹ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
full['AgeBin'] = full['AgeBin'].astype(str)
full['FareBin'] = full['FareBin'].astype(str)
print(f"  - AgeBin: {full['AgeBin'].nunique()} bins")
print(f"  - FareBin: {full['FareBin'].nunique()} bins")

# 7. Sex Ã— Pclass äº¤äº’ä½œç”¨
full['Sex_Pclass'] = full['Sex'] + '_' + full['Pclass'].astype(str)

# 8. FarePerPerson
full['FarePerPerson'] = full['Fare'] / full['FamilySize']

# =====================================
# Target Encodingï¼ˆfullãƒ™ãƒ¼ã‚¹ - ãƒªãƒ¼ã‚¯è¾¼ã¿ï¼‰
# =====================================
print("\nğŸ¯ Target encoding (leak-inclusive)...")
# trainã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’fullã«ä¸€æ™‚çš„ã«ãƒãƒ¼ã‚¸
full['Target_tmp'] = np.nan
full.loc[:train_len-1, 'Target_tmp'] = y_train.values

# ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
cat_features = ['Title', 'Embarked', 'CabinLetter', 'TicketPrefix', 'AgeBin', 'FareBin', 'Sex_Pclass']

for col in cat_features:
    # fullãƒ™ãƒ¼ã‚¹ã§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¹³å‡ã‚’è¨ˆç®—ï¼ˆãƒªãƒ¼ã‚¯ï¼ï¼‰
    target_mean = full.groupby(col)['Target_tmp'].mean()
    full[f'{col}_TE'] = full[col].map(target_mean)
    # æ¬ æå€¤ã¯å…¨ä½“å¹³å‡ã§åŸ‹ã‚ã‚‹
    full[f'{col}_TE'] = full[f'{col}_TE'].fillna(y_train.mean())
    print(f"  - {col}_TE created")

# ä¸€æ™‚çš„ãªã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã‚’å‰Šé™¤
full.drop('Target_tmp', axis=1, inplace=True)

# =====================================
# ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®Label Encoding
# =====================================
print("\nğŸ”¤ Label encoding categorical features...")
label_cols = ['Sex', 'Embarked', 'Title', 'CabinLetter', 'TicketPrefix', 'AgeBin', 'FareBin', 'Sex_Pclass']
for col in label_cols:
    le = LabelEncoder()
    full[col] = le.fit_transform(full[col].astype(str))

# =====================================
# ç‰¹å¾´é‡é¸æŠ
# =====================================
print("\nğŸ“‹ Selecting features...")
# ä½¿ç”¨ã—ãªã„åˆ—
drop_cols = ['PassengerId', 'Name', 'Ticket', 'Cabin']
feature_cols = [col for col in full.columns if col not in drop_cols]

X_full = full[feature_cols]
print(f"Final feature set: {len(feature_cols)} features")
print(f"Features: {feature_cols[:10]}... (showing first 10)")

# trainã¨testã«åˆ†å‰²
X_train = X_full[:train_len]
X_test = X_full[train_len:]

print(f"\nâœ… X_train shape: {X_train.shape}")
print(f"âœ… X_test shape: {X_test.shape}")
print(f"âœ… y_train shape: {y_train.shape}")

# =====================================
# ãƒ¢ãƒ‡ãƒ«1: GradientBoostingï¼ˆãƒ¡ã‚¤ãƒ³ï¼‰
# =====================================
print("\nğŸš€ Training GradientBoosting model...")
gb_model = GradientBoostingClassifier(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=4,
    min_samples_split=10,
    min_samples_leaf=4,
    subsample=0.8,
    random_state=42,
    verbose=0
)

# Cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(gb_model, X_train, y_train, cv=cv, scoring='accuracy')
print(f"  CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")

gb_model.fit(X_train, y_train)
train_acc = gb_model.score(X_train, y_train)
print(f"  Train Accuracy: {train_acc:.4f}")

# äºˆæ¸¬
gb_pred = gb_model.predict(X_test)
gb_pred_proba = gb_model.predict_proba(X_test)[:, 1]

# =====================================
# ãƒ¢ãƒ‡ãƒ«2: RandomForestï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç”¨ï¼‰
# =====================================
print("\nğŸŒ² Training RandomForest model...")
rf_model = RandomForestClassifier(
    n_estimators=500,
    max_depth=8,
    min_samples_split=10,
    min_samples_leaf=4,
    random_state=42,
    n_jobs=-1
)
rf_model.fit(X_train, y_train)
rf_pred_proba = rf_model.predict_proba(X_test)[:, 1]
print(f"  Train Accuracy: {rf_model.score(X_train, y_train):.4f}")

# =====================================
# ãƒ¢ãƒ‡ãƒ«3: LogisticRegressionï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç”¨ï¼‰
# =====================================
print("\nğŸ“Š Training LogisticRegression model...")
lr_model = LogisticRegression(max_iter=1000, random_state=42)
lr_model.fit(X_train, y_train)
lr_pred_proba = lr_model.predict_proba(X_test)[:, 1]
print(f"  Train Accuracy: {lr_model.score(X_train, y_train):.4f}")

# =====================================
# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼ˆweighted averageï¼‰
# =====================================
print("\nğŸ­ Ensemble predictions...")
# GBã«é‡ã¿ã‚’å¤§ããã€RFã¨LRã¯è£œåŠ©çš„ã«
ensemble_proba = (0.6 * gb_pred_proba + 0.25 * rf_pred_proba + 0.15 * lr_pred_proba)
ensemble_pred = (ensemble_proba >= 0.5).astype(int)

# =====================================
# Pseudo-labelingï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
# =====================================
print("\nğŸ”® Pseudo-labeling (optional enhancement)...")
# ç¢ºä¿¡åº¦ã®é«˜ã„äºˆæ¸¬ã‚’pseudo-labelã¨ã—ã¦åˆ©ç”¨
high_conf_idx = (ensemble_proba > 0.9) | (ensemble_proba < 0.1)
pseudo_X = X_test[high_conf_idx]
pseudo_y = ensemble_pred[high_conf_idx]

print(f"  High-confidence pseudo-labels: {len(pseudo_y)} samples")

if len(pseudo_y) > 0:
    # trainã¨pseudo-labelã‚’çµåˆã—ã¦å†å­¦ç¿’
    X_train_plus = pd.concat([X_train, pseudo_X], axis=0, ignore_index=True)
    y_train_plus = pd.concat([y_train, pd.Series(pseudo_y)], axis=0, ignore_index=True)

    print(f"  Augmented training set: {len(X_train_plus)} samples")

    # GBãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’
    gb_final = GradientBoostingClassifier(
        n_estimators=500,
        learning_rate=0.05,
        max_depth=4,
        min_samples_split=10,
        min_samples_leaf=4,
        subsample=0.8,
        random_state=42,
        verbose=0
    )
    gb_final.fit(X_train_plus, y_train_plus)

    # æœ€çµ‚äºˆæ¸¬
    final_pred = gb_final.predict(X_test)
    print(f"  Final model trained with pseudo-labeling")
else:
    final_pred = ensemble_pred
    print(f"  Using ensemble predictions (no pseudo-labeling)")

# =====================================
# çµæœä¿å­˜
# =====================================
print("\nğŸ’¾ Saving results...")
submission = pd.DataFrame({
    'PassengerId': test_ids,
    'Perished': final_pred
})
submission.to_csv('submission.csv', index=False)
print(f"  Submission saved: submission.csv")

# çµ±è¨ˆæƒ…å ±
print(f"\nğŸ“ˆ Prediction statistics:")
print(f"  Perished=0 (Survived): {(final_pred == 0).sum()} ({(final_pred == 0).mean()*100:.1f}%)")
print(f"  Perished=1 (Died): {(final_pred == 1).sum()} ({(final_pred == 1).mean()*100:.1f}%)")

# ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆä¸Šä½10å€‹ï¼‰
print(f"\nğŸ” Top 10 feature importances (GradientBoosting):")
feature_importance = pd.DataFrame({
    'feature': feature_cols,
    'importance': gb_model.feature_importances_
}).sort_values('importance', ascending=False)
print(feature_importance.head(10).to_string(index=False))

print("\nâœ… Done!")
