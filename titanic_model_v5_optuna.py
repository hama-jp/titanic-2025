#!/usr/bin/env python3
"""
Titanicã‚³ãƒ³ãƒšé¢¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ - Perishedäºˆæ¸¬ãƒ¢ãƒ‡ãƒ« v5: Optunaæœ€é©åŒ–ç‰ˆ

Overview
--------
Optunaã‚’ä½¿ç”¨ã—ãŸãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã«ã‚ˆã‚Šã€LightGBMã¨XGBoostã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’
è‡ªå‹•æ¢ç´¢ã—ã€æœ€é©ãªãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ã§äºˆæ¸¬ç²¾åº¦ã®å‘ä¸Šã‚’ç›®æŒ‡ã—ã¾ã™ã€‚

Features
--------
- **è‡ªå‹•ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢**: Optunaã«ã‚ˆã‚‹30è©¦è¡Œã®ãƒ™ã‚¤ã‚ºæœ€é©åŒ–
- **å¯¾è±¡ãƒ¢ãƒ‡ãƒ«**: LightGBM, XGBoost
- **æœ€é©åŒ–æŒ‡æ¨™**: 5-fold cross-validation accuracy
- **ç‰¹å¾´é‡**: 49å€‹ã®è©³ç´°ãªç‰¹å¾´é‡ï¼ˆv3ã¨åŒã˜æ§‹æˆï¼‰
- **ãƒªãƒ¼ã‚¯è¾¼ã¿æˆ¦ç•¥**: train+testçµåˆã«ã‚ˆã‚‹fullãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†

Optimized Hyperparameters
--------------------------
å„ãƒ¢ãƒ‡ãƒ«ã§ä»¥ä¸‹ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–:
- n_estimators: æ¨å®šå™¨ã®æ•° (500-2000)
- learning_rate: å­¦ç¿’ç‡ (0.01-0.1, log scale)
- max_depth: æœ¨ã®æœ€å¤§æ·±ã• (3-10)
- subsample: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡ (0.6-1.0)
- colsample_bytree: ç‰¹å¾´é‡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡ (0.6-1.0)
- ãã®ä»–ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (num_leaves, min_child_weightç­‰)

Workflow
--------
1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ (train.csv, test.csv)
2. train + test çµåˆ (leak-inclusive)
3. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° (49 features)
   - Title/Surname/FamilySize/Ticket/Cabinè§£æ
   - Age/Fare ãƒ“ãƒ‹ãƒ³ã‚°
   - äº¤äº’ä½œç”¨ç‰¹å¾´é‡ (SexÃ—Pclass, TitleÃ—Pclassç­‰)
   - Target encoding (leak-inclusive)
4. Optunaæœ€é©åŒ–
   - LightGBM: 30 trials
   - XGBoost: 30 trials
5. æœ€é©ãƒ¢ãƒ‡ãƒ«ã§å­¦ç¿’ãƒ»äºˆæ¸¬
6. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« (LightGBM 55% + XGBoost 45%)
7. äºˆæ¸¬çµæœä¿å­˜ (submission_v5_optuna.csv)

Usage
-----
    $ python3 titanic_model_v5_optuna.py

    Output: submission_v5_optuna.csv

Requirements
------------
- numpy
- pandas
- scikit-learn
- lightgbm
- xgboost
- optuna

Performance
-----------
- æœ€é©åŒ–ã«ã‚ˆã‚Šã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šæ¯”ã§ç²¾åº¦å‘ä¸ŠãŒæœŸå¾…ã•ã‚Œã‚‹
- å®Ÿè¡Œæ™‚é–“: ç´„5-10åˆ†ï¼ˆ30 trials Ã— 2 modelsï¼‰
- v2æ¯”è¼ƒ: v2 (0.8462) ã¨ã®æ¯”è¼ƒã«ã‚ˆã‚Šæœ€é©åŒ–åŠ¹æœã‚’è©•ä¾¡å¯èƒ½

Notes
-----
- Optunaã®æœ€é©åŒ–ã¯ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã«ä¾å­˜ã™ã‚‹ãŸã‚ã€å®Ÿè¡Œã”ã¨ã«çµæœãŒå¤‰ã‚ã‚‹å¯èƒ½æ€§ã‚ã‚Š
- ã‚ˆã‚Šå¤šãã®è©¦è¡Œæ•°ï¼ˆn_trialså¢—åŠ ï¼‰ã§ã•ã‚‰ãªã‚‹æ”¹å–„ãŒè¦‹è¾¼ã‚ã‚‹
- æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å‡ºåŠ›ãƒ­ã‚°ã«è¡¨ç¤ºã•ã‚Œã‚‹

Author
------
Generated for Titanic-style competition with leak-inclusive strategy

Version
-------
v5 - Optuna Hyperparameter Optimization
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.preprocessing import LabelEncoder
import lightgbm as lgb
import xgboost as xgb
import optuna
import warnings
warnings.filterwarnings('ignore')
optuna.logging.set_verbosity(optuna.logging.WARNING)

# =====================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# =====================================
print("ğŸ“Š Loading data...")
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

train_len = len(train)
test_ids = test['PassengerId'].copy()

# =====================================
# train + test çµåˆ
# =====================================
y_train = train['Perished'].copy()
train_drop = train.drop('Perished', axis=1)
full = pd.concat([train_drop, test], axis=0, ignore_index=True)

# =====================================
# ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆv3ã¨åŒã˜ï¼‰
# =====================================
print("\nâš™ï¸  Feature engineering v5...")

# Title
full['Title'] = full['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)
title_mapping = {
    'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',
    'Dr': 'Rare', 'Rev': 'Rare', 'Col': 'Rare', 'Major': 'Rare', 'Mlle': 'Miss',
    'Countess': 'Rare', 'Ms': 'Miss', 'Lady': 'Rare', 'Jonkheer': 'Rare',
    'Don': 'Rare', 'Dona': 'Rare', 'Mme': 'Mrs', 'Capt': 'Rare', 'Sir': 'Rare'
}
full['Title'] = full['Title'].map(title_mapping).fillna('Rare')

# Surname
full['Surname'] = full['Name'].str.split(',').str[0]
surname_freq = full.groupby('Surname')['Surname'].transform('count')
full['SurnameFreq'] = surname_freq
full['HasFamily'] = (full['SurnameFreq'] > 1).astype(int)

# FamilySize
full['FamilySize'] = full['SibSp'] + full['Parch'] + 1
full['IsAlone'] = (full['FamilySize'] == 1).astype(int)
full['FamilyCategory'] = pd.cut(full['FamilySize'], bins=[0, 1, 4, 20],
                                 labels=['Alone', 'Small', 'Large']).astype(str)
full['NameLength'] = full['Name'].apply(len)

# Ticket
full['TicketPrefix'] = full['Ticket'].str.extract('([A-Za-z/\.]+)', expand=False).fillna('NONE')
ticket_counts = full['TicketPrefix'].value_counts()
full['TicketPrefix'] = full['TicketPrefix'].apply(lambda x: x if ticket_counts[x] >= 5 else 'RARE')
full['TicketNumber'] = pd.to_numeric(full['Ticket'].str.extract('(\d+)', expand=False), errors='coerce').fillna(0)
full['TicketFreq'] = full.groupby('Ticket')['Ticket'].transform('count')

# Cabin
full['CabinLetter'] = full['Cabin'].str[0].fillna('X')
full['HasCabin'] = (full['Cabin'].notna()).astype(int)
full['CabinCount'] = full['Cabin'].fillna('').apply(lambda x: len(x.split()))

# æ¬ æå€¤å‡¦ç†
full['Age'] = full.groupby(['Title', 'Pclass'])['Age'].transform(lambda x: x.fillna(x.median()))
full['Age'] = full.groupby('Title')['Age'].transform(lambda x: x.fillna(x.median()))
full['Fare'] = full.groupby('Pclass')['Fare'].transform(lambda x: x.fillna(x.median()))
full['Embarked'] = full['Embarked'].fillna(full['Embarked'].mode()[0])

# ãƒ“ãƒ‹ãƒ³ã‚°
full['AgeBin'] = pd.cut(full['Age'], bins=[0, 12, 18, 35, 60, 100],
                        labels=['Child', 'Teen', 'Adult', 'Middle', 'Senior']).astype(str)
full['AgeBin_Fine'] = pd.cut(full['Age'], bins=10, labels=False)
full['FareBin'] = pd.qcut(full['Fare'], q=5, labels=['VeryLow', 'Low', 'Med', 'High', 'VeryHigh'],
                          duplicates='drop').astype(str)
full['FareBin_Fine'] = pd.qcut(full['Fare'], q=10, labels=False, duplicates='drop')

# äº¤äº’ä½œç”¨
full['Sex_Pclass'] = full['Sex'] + '_' + full['Pclass'].astype(str)
full['Title_Pclass'] = full['Title'] + '_' + full['Pclass'].astype(str)
full['Age_Pclass'] = full['AgeBin'] + '_' + full['Pclass'].astype(str)
full['Embarked_Pclass'] = full['Embarked'] + '_' + full['Pclass'].astype(str)
full['Title_Sex'] = full['Title'] + '_' + full['Sex']

# æ´¾ç”Ÿæ•°å€¤
full['FarePerPerson'] = full['Fare'] / full['FamilySize']
full['Age_Times_Class'] = full['Age'] * full['Pclass']
full['Fare_Times_Class'] = full['Fare'] * full['Pclass']
full['Age_Sex'] = full['Age'] * full['Sex'].map({'male': 1, 'female': 0})
full['Age_Fare_Interaction'] = full['Age'] * full['Fare']
full['SibSp_Parch_Interaction'] = full['SibSp'] * full['Parch']

# Fareç•°å¸¸å€¤
fare_mean = full['Fare'].mean()
fare_std = full['Fare'].std()
full['FareOutlier'] = (full['Fare'] > fare_mean + 2 * fare_std).astype(int)

# Target Encoding
full['Target_tmp'] = np.nan
full.loc[:train_len-1, 'Target_tmp'] = y_train.values

cat_features = [
    'Title', 'Embarked', 'CabinLetter', 'TicketPrefix',
    'AgeBin', 'FareBin', 'Sex_Pclass', 'Title_Pclass',
    'Age_Pclass', 'FamilyCategory', 'Embarked_Pclass', 'Title_Sex'
]

for col in cat_features:
    target_mean = full.groupby(col)['Target_tmp'].mean()
    full[f'{col}_TE'] = full[col].map(target_mean).fillna(y_train.mean())

full.drop('Target_tmp', axis=1, inplace=True)

# Label Encoding
label_cols = [
    'Sex', 'Embarked', 'Title', 'CabinLetter', 'TicketPrefix',
    'AgeBin', 'FareBin', 'Sex_Pclass', 'Title_Pclass', 'Age_Pclass',
    'FamilyCategory', 'Embarked_Pclass', 'Title_Sex', 'Surname'
]
for col in label_cols:
    le = LabelEncoder()
    full[col] = le.fit_transform(full[col].astype(str))

# ç‰¹å¾´é‡é¸æŠ
drop_cols = ['PassengerId', 'Name', 'Ticket', 'Cabin']
feature_cols = [col for col in full.columns if col not in drop_cols]

X_full = full[feature_cols]
X_train = X_full[:train_len]
X_test = X_full[train_len:]

print(f"Final feature set: {len(feature_cols)} features")

# =====================================
# Optuna: LightGBMæœ€é©åŒ–
# =====================================
print("\nğŸ” Optuna: Optimizing LightGBM hyperparameters...")

def objective_lgb(trial):
    """
    LightGBMç”¨ã®Optunaç›®çš„é–¢æ•°

    ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã«ã‚ˆã‚Šã€LightGBMã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ç´¢ã—ã€
    5-fold cross-validationã®accuracyã‚’æœ€å¤§åŒ–ã™ã‚‹ã€‚

    Parameters
    ----------
    trial : optuna.trial.Trial
        Optunaã®ãƒˆãƒ©ã‚¤ã‚¢ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ææ¡ˆã«ä½¿ç”¨ã€‚

    Returns
    -------
    float
        5-fold cross-validationã®å¹³å‡accuracy

    Optimized Parameters
    --------------------
    - n_estimators: æ¨å®šå™¨ã®æ•° [500, 2000] (step=100)
    - learning_rate: å­¦ç¿’ç‡ [0.01, 0.1] (log scale)
    - max_depth: æœ¨ã®æœ€å¤§æ·±ã• [3, 10]
    - num_leaves: è‘‰ãƒãƒ¼ãƒ‰æ•° [15, 63]
    - min_child_samples: å­ãƒãƒ¼ãƒ‰ã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•° [5, 30]
    - subsample: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡ [0.6, 1.0]
    - colsample_bytree: ç‰¹å¾´é‡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡ [0.6, 1.0]

    Notes
    -----
    - 5-fold StratifiedKFoldã§è©•ä¾¡
    - random_state=42ã§å†ç¾æ€§ã‚’ç¢ºä¿
    - verbose=-1ã§è­¦å‘ŠæŠ‘åˆ¶
    """
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 500, 2000, step=100),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'num_leaves': trial.suggest_int('num_leaves', 15, 63),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 30),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'random_state': 42,
        'verbose': -1
    }

    model = lgb.LGBMClassifier(**params)
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1)
    return scores.mean()

study_lgb = optuna.create_study(direction='maximize', study_name='lgb_optim')
study_lgb.optimize(objective_lgb, n_trials=30, show_progress_bar=False)

print(f"  Best LightGBM CV Accuracy: {study_lgb.best_value:.4f}")
print(f"  Best params: {study_lgb.best_params}")

# =====================================
# Optuna: XGBoostæœ€é©åŒ–
# =====================================
print("\nğŸ” Optuna: Optimizing XGBoost hyperparameters...")

def objective_xgb(trial):
    """
    XGBoostç”¨ã®Optunaç›®çš„é–¢æ•°

    ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã«ã‚ˆã‚Šã€XGBoostã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ç´¢ã—ã€
    5-fold cross-validationã®accuracyã‚’æœ€å¤§åŒ–ã™ã‚‹ã€‚

    Parameters
    ----------
    trial : optuna.trial.Trial
        Optunaã®ãƒˆãƒ©ã‚¤ã‚¢ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ææ¡ˆã«ä½¿ç”¨ã€‚

    Returns
    -------
    float
        5-fold cross-validationã®å¹³å‡accuracy

    Optimized Parameters
    --------------------
    - n_estimators: æ¨å®šå™¨ã®æ•° [500, 2000] (step=100)
    - learning_rate: å­¦ç¿’ç‡ [0.01, 0.1] (log scale)
    - max_depth: æœ¨ã®æœ€å¤§æ·±ã• [3, 10]
    - min_child_weight: å­ãƒãƒ¼ãƒ‰ã®æœ€å°é‡ã¿ [1, 10]
    - subsample: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡ [0.6, 1.0]
    - colsample_bytree: ç‰¹å¾´é‡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡ [0.6, 1.0]

    Notes
    -----
    - 5-fold StratifiedKFoldã§è©•ä¾¡
    - random_state=42ã§å†ç¾æ€§ã‚’ç¢ºä¿
    - eval_metric='logloss'ã§æå¤±é–¢æ•°ã‚’æŒ‡å®š
    - verbosity=0ã§è­¦å‘ŠæŠ‘åˆ¶
    """
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 500, 2000, step=100),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'random_state': 42,
        'eval_metric': 'logloss',
        'verbosity': 0
    }

    model = xgb.XGBClassifier(**params)
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1)
    return scores.mean()

study_xgb = optuna.create_study(direction='maximize', study_name='xgb_optim')
study_xgb.optimize(objective_xgb, n_trials=30, show_progress_bar=False)

print(f"  Best XGBoost CV Accuracy: {study_xgb.best_value:.4f}")
print(f"  Best params: {study_xgb.best_params}")

# =====================================
# æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§å­¦ç¿’ãƒ»äºˆæ¸¬
# =====================================
print("\nğŸš€ Training optimized models...")

# Best LightGBM
best_lgb = lgb.LGBMClassifier(**study_lgb.best_params)
best_lgb.fit(X_train, y_train)
lgb_pred = best_lgb.predict_proba(X_test)[:, 1]

# Best XGBoost
best_xgb = xgb.XGBClassifier(**study_xgb.best_params)
best_xgb.fit(X_train, y_train)
xgb_pred = best_xgb.predict_proba(X_test)[:, 1]

# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
ensemble_proba = (0.55 * lgb_pred + 0.45 * xgb_pred)
final_pred = (ensemble_proba >= 0.5).astype(int)

# =====================================
# çµæœä¿å­˜
# =====================================
print("\nğŸ’¾ Saving results...")
submission = pd.DataFrame({
    'PassengerId': test_ids,
    'Perished': final_pred
})
submission.to_csv('submission_v5_optuna.csv', index=False)
print(f"  Submission saved: submission_v5_optuna.csv")

print(f"\nğŸ“ˆ Prediction statistics:")
print(f"  Perished=0 (Survived): {(final_pred == 0).sum()} ({(final_pred == 0).mean()*100:.1f}%)")
print(f"  Perished=1 (Died): {(final_pred == 1).sum()} ({(final_pred == 1).mean()*100:.1f}%)")

print("\nâœ… Done! v5 with Optuna hyperparameter optimization")
print(f"  LightGBM best CV: {study_lgb.best_value:.4f}")
print(f"  XGBoost best CV: {study_xgb.best_value:.4f}")
