#!/usr/bin/env python3
"""
Phase 1-1: v1ã¨v3ã®ãƒ–ãƒ¬ãƒ³ãƒ‰æœ€é©åŒ–

æ—¢å­˜ã®submissionãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã€æœ€é©ãªã‚¦ã‚§ã‚¤ãƒˆã‚’æŽ¢ç´¢ã—ã¾ã™ã€‚
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import LabelEncoder
import itertools
import warnings
warnings.filterwarnings('ignore')

print("=" * 70)
print("Phase 1-1: ãƒ–ãƒ¬ãƒ³ãƒ‰æœ€é©åŒ–")
print("=" * 70)

# =====================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†ï¼ˆv1ã¨åŒã˜ï¼‰
# =====================================
print("\nðŸ“Š Loading data and preparing features...")
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

train_len = len(train)
y_train = train['Perished'].copy()
train_drop = train.drop('Perished', axis=1)
full = pd.concat([train_drop, test], axis=0, ignore_index=True)

# ç°¡æ˜“ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆv1ãƒ™ãƒ¼ã‚¹ï¼‰
full['Title'] = full['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)
title_mapping = {
    'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',
    'Dr': 'Rare', 'Rev': 'Rare', 'Col': 'Rare', 'Major': 'Rare', 'Mlle': 'Miss',
    'Countess': 'Rare', 'Ms': 'Miss', 'Lady': 'Rare', 'Jonkheer': 'Rare',
    'Don': 'Rare', 'Dona': 'Rare', 'Mme': 'Mrs', 'Capt': 'Rare', 'Sir': 'Rare'
}
full['Title'] = full['Title'].map(title_mapping).fillna('Rare')

full['FamilySize'] = full['SibSp'] + full['Parch'] + 1
full['IsAlone'] = (full['FamilySize'] == 1).astype(int)

full['TicketPrefix'] = full['Ticket'].str.extract('([A-Za-z/\.]+)', expand=False).fillna('NONE')
ticket_counts = full['TicketPrefix'].value_counts()
full['TicketPrefix'] = full['TicketPrefix'].apply(lambda x: x if ticket_counts[x] >= 5 else 'RARE')

full['CabinLetter'] = full['Cabin'].str[0].fillna('X')
full['HasCabin'] = (full['Cabin'].notna()).astype(int)

full['Age'] = full.groupby('Title')['Age'].transform(lambda x: x.fillna(x.median()))
full['Fare'] = full.groupby('Pclass')['Fare'].transform(lambda x: x.fillna(x.median()))
full['Embarked'] = full['Embarked'].fillna(full['Embarked'].mode()[0])

full['AgeBin'] = pd.cut(full['Age'], bins=[0, 12, 18, 35, 60, 100],
                        labels=['Child', 'Teen', 'Adult', 'Middle', 'Senior']).astype(str)
full['FareBin'] = pd.qcut(full['Fare'], q=5, labels=['VeryLow', 'Low', 'Med', 'High', 'VeryHigh'],
                          duplicates='drop').astype(str)

full['Sex_Pclass'] = full['Sex'] + '_' + full['Pclass'].astype(str)
full['FarePerPerson'] = full['Fare'] / full['FamilySize']

# Target Encoding
full['Target_tmp'] = np.nan
full.loc[:train_len-1, 'Target_tmp'] = y_train.values

for col in ['Title', 'Embarked', 'CabinLetter', 'TicketPrefix', 'AgeBin', 'FareBin', 'Sex_Pclass']:
    target_mean = full.groupby(col)['Target_tmp'].mean()
    full[f'{col}_TE'] = full[col].map(target_mean).fillna(y_train.mean())

full.drop('Target_tmp', axis=1, inplace=True)

# Label Encoding
for col in ['Sex', 'Embarked', 'Title', 'CabinLetter', 'TicketPrefix', 'AgeBin', 'FareBin', 'Sex_Pclass']:
    le = LabelEncoder()
    full[col] = le.fit_transform(full[col].astype(str))

drop_cols = ['PassengerId', 'Name', 'Ticket', 'Cabin']
feature_cols = [col for col in full.columns if col not in drop_cols]

X_full = full[feature_cols]
X_train = X_full[:train_len]
X_test = X_full[train_len:]

print(f"Feature set: {len(feature_cols)} features")

# =====================================
# ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆv1ã¨v3ã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
# =====================================
print("\nðŸ”§ Training models for blend optimization...")

# ãƒ¢ãƒ‡ãƒ«1: GradientBoostingï¼ˆv1ã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

gb1 = GradientBoostingClassifier(n_estimators=500, learning_rate=0.05, max_depth=4,
                                 min_samples_split=10, min_samples_leaf=4, subsample=0.8,
                                 random_state=42)
rf1 = RandomForestClassifier(n_estimators=500, max_depth=8, min_samples_split=10,
                             min_samples_leaf=4, random_state=42, n_jobs=-1)
lr1 = LogisticRegression(max_iter=1000, random_state=42)

gb1.fit(X_train, y_train)
rf1.fit(X_train, y_train)
lr1.fit(X_train, y_train)

# v1ã‚¹ã‚¿ã‚¤ãƒ«ã®äºˆæ¸¬ç¢ºçŽ‡
v1_pred_proba = (0.6 * gb1.predict_proba(X_test)[:, 1] +
                 0.25 * rf1.predict_proba(X_test)[:, 1] +
                 0.15 * lr1.predict_proba(X_test)[:, 1])

# ãƒ¢ãƒ‡ãƒ«2: LightGBMï¼ˆv3ã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
import lightgbm as lgb

lgb_model = lgb.LGBMClassifier(n_estimators=1000, learning_rate=0.02, max_depth=6,
                               num_leaves=31, min_child_samples=10, subsample=0.8,
                               colsample_bytree=0.8, random_state=42, verbose=-1)
lgb_model.fit(X_train, y_train)

# v3ã‚¹ã‚¿ã‚¤ãƒ«ã®äºˆæ¸¬ç¢ºçŽ‡
v3_pred_proba = lgb_model.predict_proba(X_test)[:, 1]

print("  v1-style model trained")
print("  v3-style model (LightGBM) trained")

# =====================================
# ã‚¦ã‚§ã‚¤ãƒˆæœ€é©åŒ–ï¼ˆCVãƒ™ãƒ¼ã‚¹ï¼‰
# =====================================
print("\nðŸ” Optimizing blend weights using CV...")

# OOFï¼ˆOut-of-Foldï¼‰äºˆæ¸¬ã‚’ç”Ÿæˆ
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
oof_v1 = np.zeros(len(X_train))
oof_v3 = np.zeros(len(X_train))

for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):
    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]

    # v1ã‚¹ã‚¿ã‚¤ãƒ«
    gb_fold = GradientBoostingClassifier(n_estimators=500, learning_rate=0.05, max_depth=4,
                                         min_samples_split=10, min_samples_leaf=4,
                                         subsample=0.8, random_state=42)
    rf_fold = RandomForestClassifier(n_estimators=500, max_depth=8, min_samples_split=10,
                                     min_samples_leaf=4, random_state=42, n_jobs=-1)
    lr_fold = LogisticRegression(max_iter=1000, random_state=42)

    gb_fold.fit(X_tr, y_tr)
    rf_fold.fit(X_tr, y_tr)
    lr_fold.fit(X_tr, y_tr)

    oof_v1[val_idx] = (0.6 * gb_fold.predict_proba(X_val)[:, 1] +
                       0.25 * rf_fold.predict_proba(X_val)[:, 1] +
                       0.15 * lr_fold.predict_proba(X_val)[:, 1])

    # v3ã‚¹ã‚¿ã‚¤ãƒ«
    lgb_fold = lgb.LGBMClassifier(n_estimators=1000, learning_rate=0.02, max_depth=6,
                                  num_leaves=31, min_child_samples=10, subsample=0.8,
                                  colsample_bytree=0.8, random_state=42, verbose=-1)
    lgb_fold.fit(X_tr, y_tr)
    oof_v3[val_idx] = lgb_fold.predict_proba(X_val)[:, 1]

print("  OOF predictions generated")

# ã‚¦ã‚§ã‚¤ãƒˆæŽ¢ç´¢
best_weight = 0.5
best_acc = 0.0

print("\n  Testing weight combinations:")
for w1 in np.arange(0, 1.05, 0.05):
    w3 = 1.0 - w1
    blend_pred = w1 * oof_v1 + w3 * oof_v3
    blend_binary = (blend_pred >= 0.5).astype(int)
    acc = (blend_binary == y_train).mean()

    if acc > best_acc:
        best_acc = acc
        best_weight = w1

    if w1 in [0.0, 0.25, 0.5, 0.75, 1.0]:
        print(f"    v1:{w1:.2f} + v3:{w3:.2f} = Accuracy: {acc:.4f}")

print(f"\n  âœ… Best blend: v1 {best_weight:.2f} + v3 {1-best_weight:.2f}")
print(f"  âœ… Best CV Accuracy: {best_acc:.4f}")

# =====================================
# æœ€çµ‚äºˆæ¸¬
# =====================================
print("\nðŸŽ¯ Generating final predictions with optimized weights...")

final_proba = best_weight * v1_pred_proba + (1 - best_weight) * v3_pred_proba
final_pred = (final_proba >= 0.5).astype(int)

# ä¿å­˜
test_ids = test['PassengerId']
submission = pd.DataFrame({
    'PassengerId': test_ids,
    'Perished': final_pred
})
submission.to_csv('submission_phase1_blend.csv', index=False)

print(f"  Saved: submission_phase1_blend.csv")
print(f"\nðŸ“ˆ Prediction statistics:")
print(f"  Perished=0: {(final_pred == 0).sum()} ({(final_pred == 0).mean()*100:.1f}%)")
print(f"  Perished=1: {(final_pred == 1).sum()} ({(final_pred == 1).mean()*100:.1f}%)")

# æ¯”è¼ƒ
print(f"\nðŸ“Š Comparison with baseline:")
v1_alone = (v1_pred_proba >= 0.5).astype(int)
v3_alone = (v3_pred_proba >= 0.5).astype(int)

v1_oof_acc = ((oof_v1 >= 0.5).astype(int) == y_train).mean()
v3_oof_acc = ((oof_v3 >= 0.5).astype(int) == y_train).mean()

print(f"  v1 alone CV: {v1_oof_acc:.4f}")
print(f"  v3 alone CV: {v3_oof_acc:.4f}")
print(f"  Optimized blend CV: {best_acc:.4f}")
print(f"  Improvement: +{(best_acc - max(v1_oof_acc, v3_oof_acc)):.4f}")

print("\nâœ… Phase 1-1 Complete!")
