#!/usr/bin/env python3
"""
Phase 1-2: ç‰¹å¾´é‡é‡è¦åº¦ã«ã‚ˆã‚‹é¸æŠ

v3ã®49ç‰¹å¾´é‡ã‹ã‚‰é‡è¦åº¦ã®ä½ã„ã‚‚ã®ã‚’å‰Šé™¤ã—ã¦éå­¦ç¿’ã‚’æŠ‘åˆ¶ã—ã¾ã™ã€‚
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.preprocessing import LabelEncoder
import lightgbm as lgb
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

print("=" * 70)
print("Phase 1-2: ç‰¹å¾´é‡é¸æŠ")
print("=" * 70)

# =====================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆv3ãƒ™ãƒ¼ã‚¹ï¼‰
# =====================================
print("\nğŸ“Š Loading data and creating features (v3-style)...")
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

train_len = len(train)
test_ids = test['PassengerId'].copy()
y_train = train['Perished'].copy()
train_drop = train.drop('Perished', axis=1)
full = pd.concat([train_drop, test], axis=0, ignore_index=True)

# v3ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆ49ç‰¹å¾´é‡ï¼‰
full['Title'] = full['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)
title_mapping = {
    'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',
    'Dr': 'Rare', 'Rev': 'Rare', 'Col': 'Rare', 'Major': 'Rare', 'Mlle': 'Miss',
    'Countess': 'Rare', 'Ms': 'Miss', 'Lady': 'Rare', 'Jonkheer': 'Rare',
    'Don': 'Rare', 'Dona': 'Rare', 'Mme': 'Mrs', 'Capt': 'Rare', 'Sir': 'Rare'
}
full['Title'] = full['Title'].map(title_mapping).fillna('Rare')

full['Surname'] = full['Name'].str.split(',').str[0]
surname_freq = full.groupby('Surname')['Surname'].transform('count')
full['SurnameFreq'] = surname_freq
full['HasFamily'] = (full['SurnameFreq'] > 1).astype(int)

full['FamilySize'] = full['SibSp'] + full['Parch'] + 1
full['IsAlone'] = (full['FamilySize'] == 1).astype(int)
full['FamilyCategory'] = pd.cut(full['FamilySize'], bins=[0, 1, 4, 20],
                                labels=['Alone', 'Small', 'Large']).astype(str)
full['NameLength'] = full['Name'].apply(len)

full['TicketPrefix'] = full['Ticket'].str.extract('([A-Za-z/\.]+)', expand=False).fillna('NONE')
ticket_counts = full['TicketPrefix'].value_counts()
full['TicketPrefix'] = full['TicketPrefix'].apply(lambda x: x if ticket_counts[x] >= 5 else 'RARE')
full['TicketNumber'] = pd.to_numeric(full['Ticket'].str.extract('(\d+)', expand=False), errors='coerce').fillna(0)
full['TicketFreq'] = full.groupby('Ticket')['Ticket'].transform('count')

full['CabinLetter'] = full['Cabin'].str[0].fillna('X')
full['HasCabin'] = (full['Cabin'].notna()).astype(int)
full['CabinCount'] = full['Cabin'].fillna('').apply(lambda x: len(x.split()))

full['Age'] = full.groupby(['Title', 'Pclass'])['Age'].transform(lambda x: x.fillna(x.median()))
full['Age'] = full.groupby('Title')['Age'].transform(lambda x: x.fillna(x.median()))
full['Fare'] = full.groupby('Pclass')['Fare'].transform(lambda x: x.fillna(x.median()))
full['Embarked'] = full['Embarked'].fillna(full['Embarked'].mode()[0])

full['AgeBin'] = pd.cut(full['Age'], bins=[0, 12, 18, 35, 60, 100],
                       labels=['Child', 'Teen', 'Adult', 'Middle', 'Senior']).astype(str)
full['AgeBin_Fine'] = pd.cut(full['Age'], bins=10, labels=False)
full['FareBin'] = pd.qcut(full['Fare'], q=5, labels=['VeryLow', 'Low', 'Med', 'High', 'VeryHigh'],
                          duplicates='drop').astype(str)
full['FareBin_Fine'] = pd.qcut(full['Fare'], q=10, labels=False, duplicates='drop')

full['Sex_Pclass'] = full['Sex'] + '_' + full['Pclass'].astype(str)
full['Title_Pclass'] = full['Title'] + '_' + full['Pclass'].astype(str)
full['Age_Pclass'] = full['AgeBin'] + '_' + full['Pclass'].astype(str)
full['Embarked_Pclass'] = full['Embarked'] + '_' + full['Pclass'].astype(str)
full['Title_Sex'] = full['Title'] + '_' + full['Sex']

full['FarePerPerson'] = full['Fare'] / full['FamilySize']
full['Age_Times_Class'] = full['Age'] * full['Pclass']
full['Fare_Times_Class'] = full['Fare'] * full['Pclass']
full['Age_Sex'] = full['Age'] * full['Sex'].map({'male': 1, 'female': 0})
full['Age_Fare_Interaction'] = full['Age'] * full['Fare']
full['SibSp_Parch_Interaction'] = full['SibSp'] * full['Parch']

fare_mean = full['Fare'].mean()
fare_std = full['Fare'].std()
full['FareOutlier'] = (full['Fare'] > fare_mean + 2 * fare_std).astype(int)

# Target Encoding
full['Target_tmp'] = np.nan
full.loc[:train_len-1, 'Target_tmp'] = y_train.values

cat_features = [
    'Title', 'Embarked', 'CabinLetter', 'TicketPrefix',
    'AgeBin', 'FareBin', 'Sex_Pclass', 'Title_Pclass',
    'Age_Pclass', 'FamilyCategory', 'Embarked_Pclass', 'Title_Sex'
]

for col in cat_features:
    target_mean = full.groupby(col)['Target_tmp'].mean()
    full[f'{col}_TE'] = full[col].map(target_mean).fillna(y_train.mean())

full.drop('Target_tmp', axis=1, inplace=True)

# Label Encoding
label_cols = [
    'Sex', 'Embarked', 'Title', 'CabinLetter', 'TicketPrefix',
    'AgeBin', 'FareBin', 'Sex_Pclass', 'Title_Pclass', 'Age_Pclass',
    'FamilyCategory', 'Embarked_Pclass', 'Title_Sex', 'Surname'
]
for col in label_cols:
    le = LabelEncoder()
    full[col] = le.fit_transform(full[col].astype(str))

drop_cols = ['PassengerId', 'Name', 'Ticket', 'Cabin']
feature_cols = [col for col in full.columns if col not in drop_cols]

X_full = full[feature_cols]
X_train = X_full[:train_len]
X_test = X_full[train_len:]

print(f"Original features: {len(feature_cols)}")

# =====================================
# ç‰¹å¾´é‡é‡è¦åº¦ã®è¨ˆç®—
# =====================================
print("\nğŸ” Calculating feature importances...")

# LightGBMã§ç‰¹å¾´é‡é‡è¦åº¦ã‚’è¨ˆç®—
lgb_model = lgb.LGBMClassifier(
    n_estimators=1000,
    learning_rate=0.02,
    max_depth=6,
    num_leaves=31,
    min_child_samples=10,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbose=-1
)
lgb_model.fit(X_train, y_train)

# ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—
importance_df = pd.DataFrame({
    'feature': feature_cols,
    'importance': lgb_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\nTop 15 most important features:")
print(importance_df.head(15).to_string(index=False))

# =====================================
# ç‰¹å¾´é‡é¸æŠï¼ˆè¤‡æ•°ã®é–¾å€¤ã§è©¦ã™ï¼‰
# =====================================
print("\nğŸ“Š Testing feature selection with different thresholds...")

# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³CV
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
baseline_scores = cross_val_score(lgb_model, X_train, y_train, cv=cv, scoring='accuracy')
baseline_cv = baseline_scores.mean()

print(f"\nBaseline (all {len(feature_cols)} features):")
print(f"  CV Accuracy: {baseline_cv:.4f} (+/- {baseline_scores.std():.4f})")

results = []
results.append({
    'n_features': len(feature_cols),
    'cv_mean': baseline_cv,
    'cv_std': baseline_scores.std(),
    'selected_features': feature_cols
})

# ç•°ãªã‚‹ç‰¹å¾´é‡æ•°ã§è©¦ã™
for keep_ratio in [0.9, 0.8, 0.7, 0.6]:
    n_keep = int(len(feature_cols) * keep_ratio)
    selected_features = importance_df.head(n_keep)['feature'].tolist()

    X_train_selected = X_train[selected_features]

    lgb_selected = lgb.LGBMClassifier(
        n_estimators=1000,
        learning_rate=0.02,
        max_depth=6,
        num_leaves=31,
        min_child_samples=10,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        verbose=-1
    )

    scores = cross_val_score(lgb_selected, X_train_selected, y_train, cv=cv, scoring='accuracy')
    cv_mean = scores.mean()
    cv_std = scores.std()

    results.append({
        'n_features': n_keep,
        'cv_mean': cv_mean,
        'cv_std': cv_std,
        'selected_features': selected_features
    })

    print(f"\nTop {n_keep} features ({keep_ratio*100:.0f}%):")
    print(f"  CV Accuracy: {cv_mean:.4f} (+/- {cv_std:.4f})")
    print(f"  Change: {cv_mean - baseline_cv:+.4f}")

# æœ€è‰¯ã®çµæœã‚’é¸æŠ
best_result = max(results, key=lambda x: x['cv_mean'])

print(f"\nâœ… Best configuration:")
print(f"  Features: {best_result['n_features']}")
print(f"  CV Accuracy: {best_result['cv_mean']:.4f}")
print(f"  Improvement: {best_result['cv_mean'] - baseline_cv:+.4f}")

# =====================================
# æœ€è‰¯ã®ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã§æœ€çµ‚äºˆæ¸¬
# =====================================
print("\nğŸ¯ Training final model with selected features...")

selected_features = best_result['selected_features']
X_train_final = X_train[selected_features]
X_test_final = X_test[selected_features]

lgb_final = lgb.LGBMClassifier(
    n_estimators=1000,
    learning_rate=0.02,
    max_depth=6,
    num_leaves=31,
    min_child_samples=10,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbose=-1
)
lgb_final.fit(X_train_final, y_train)

final_pred = lgb_final.predict(X_test_final)

# ä¿å­˜
submission = pd.DataFrame({
    'PassengerId': test_ids,
    'Perished': final_pred
})
submission.to_csv('submission_phase1_feature_selection.csv', index=False)

print(f"  Saved: submission_phase1_feature_selection.csv")
print(f"\nğŸ“ˆ Prediction statistics:")
print(f"  Perished=0: {(final_pred == 0).sum()} ({(final_pred == 0).mean()*100:.1f}%)")
print(f"  Perished=1: {(final_pred == 1).sum()} ({(final_pred == 1).mean()*100:.1f}%)")

print(f"\nSelected {len(selected_features)} features:")
print(", ".join(selected_features[:20]), "...")

print("\nâœ… Phase 1-2 Complete!")
